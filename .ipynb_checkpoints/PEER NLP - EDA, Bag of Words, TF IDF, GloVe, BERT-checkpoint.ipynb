{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0\"></a>\n",
    "# [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started)\n",
    "\n",
    "# NLP:\n",
    "* EDA (with WordCloud) \n",
    "* Bag of Words \n",
    "* TF IDF\n",
    "* GloVe\n",
    "* BERT with TFHub and with Submission\n",
    "* PCA visualization\n",
    "* Showing Confusion Matrix for BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n",
    "\n",
    "This kernel uses such good kernels: \n",
    "* https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n",
    "* https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n",
    "* https://www.kaggle.com/itratrahman/nlp-tutorial-using-python\n",
    "* https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud\n",
    "* https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic\n",
    "* https://www.kaggle.com/khoongweihao/bert-base-tf2-0-minimalistic-iii\n",
    "* https://www.kaggle.com/vbmokin/disaster-nlp-keras-bert-using-tfhub-tuning\n",
    "* https://www.kaggle.com/user123454321/bert-starter-inference\n",
    "* https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n",
    "* https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data\n",
    "\n",
    "and other resources:\n",
    "* https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb\n",
    "* https://tfhub.dev/s?q=bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "1. [My upgrade BERT model](#1)\n",
    "    -  [Commit now](#1.1)\n",
    "    -  [Previous commits: Dropout = 0.1 or 0.3](#1.2)\n",
    "    -  [Previous commits: epochs = 3](#1.3)\n",
    "    -  [Previous commits: epochs = 4](#1.4)\n",
    "    -  [Previous commits: epochs = 5](#1.5)\n",
    "    -  [Previous commits: with training tweets correction](#1.6)\n",
    "    -  [Previous commits: parameters and LB scores](#1.7)    \n",
    "1. [Import libraries](#2)\n",
    "1. [Download data](#3)\n",
    "1. [EDA](#4)\n",
    "1. [Data Cleaning](#5)\n",
    "1. [WordCloud](#6)\n",
    "1. [Bag of Words Counts](#7)\n",
    "1. [TF IDF](#8)\n",
    "1. [GloVe](#9)\n",
    "1. [BERT using TFHub](#10)\n",
    "   - [Submission by BERT](#10.1)\n",
    "1. [Showing Confusion Matrices](#11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. My upgrade BERT model <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Commit now <a class=\"anchor\" id=\"1.1\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* without Dropout\n",
    "* Adam(lr=2e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 5,\n",
    "* batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Previous commits: Dropout = 0.1 or 0.3 <a class=\"anchor\" id=\"1.2\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 24\n",
    "* Dropout(0.3)\n",
    "* Adam(lr=2e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 4,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.80470**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 30\n",
    "* Dropout(0.1)\n",
    "* Adam(lr=1e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 4,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.83333**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Previous commits: epochs = 3 <a class=\"anchor\" id=\"1.3\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 14\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=2e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 3,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.83128**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 37\n",
    "#### From https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub (commit 2)\n",
    "* without Dropout\n",
    "* Adam(lr=2e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 3,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.81390** - This is strange, since this original model gave LB = 0.84355"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Previous commits: epochs = 4 <a class=\"anchor\" id=\"1.4\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 15\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=2e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 4,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.83742**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 25\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=3e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 4,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.83537**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 26\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=4e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 4,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.82617**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 27\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=5e-4)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 4,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.57055**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 28\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=5e-6)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 4,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.82924**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 29\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=1e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 4,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.83742**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 31\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=1e-5)\n",
    "* validation_split = 0.15,\n",
    "* epochs = 4,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.79856**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 33\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=2e-5)\n",
    "* validation_split = 0.3,\n",
    "* epochs = 4,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.83128**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 35\n",
    "* Dropout(0.15)\n",
    "* Adam(lr=2e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 4,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.81390**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Previous commits: epochs = 5 <a class=\"anchor\" id=\"1.5\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 23\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=2e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 5,\n",
    "* batch_size = 32\n",
    "\n",
    "**LB = 0.83435**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 39\n",
    "* without Dropout\n",
    "* Adam(lr=2e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 4,\n",
    "* batch_size = 24\n",
    "\n",
    "**LB = 0.83435**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Previous commits: with training tweets correction <a class=\"anchor\" id=\"1.6\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 18\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=2e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 3,\n",
    "* batch_size = 16\n",
    "\n",
    "From https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data - author of this kernel read tweets in training data and figure out that some of them have errors:\n",
    "\n",
    "    ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "    train.loc[train['id'].isin(ids_with_target_error),'target'] = 0\n",
    "\n",
    "**LB = 0.83537**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 20\n",
    "* without Dropout\n",
    "* Adam(lr=1e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 3,\n",
    "* batch_size = 16\n",
    "\n",
    "From https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data - author of this kernel read tweets in training data and figure out that some of them have errors:\n",
    "\n",
    "    ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "    train.loc[train['id'].isin(ids_with_target_error),'target'] = 0\n",
    "\n",
    "**LB = 0.82413**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 22\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=1.1e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 3,\n",
    "* batch_size = 16\n",
    "\n",
    "From https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data - author of this kernel read tweets in training data and figure out that some of them have errors:\n",
    "\n",
    "    ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "    train.loc[train['id'].isin(ids_with_target_error),'target'] = 0\n",
    "\n",
    "**LB (for BERT) = 0.80879**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 32\n",
    "\n",
    "* Dropout(0.2)\n",
    "* Adam(lr=2e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 4,\n",
    "* batch_size = 32\n",
    "\n",
    "From https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data - author of this kernel read tweets in training data and figure out that some of them have errors:\n",
    "\n",
    "    ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "    train.loc[train['id'].isin(ids_with_target_error),'target'] = 0\n",
    "\n",
    "**LB = 0.82004**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Previous commits: parameters and LB scores<a class=\"anchor\" id=\"1.7\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Commit 15\n",
    "* Dropout=0.2/lr=2e-5/val=0.2/epochs=4/batch=32/anomall=False\n",
    "* LB = **0.83742**\n",
    "* 29\n",
    "* Dropout=0.2/lr=1e-5/val=0.2/epochs=4/batch=32/anomall=False\n",
    "* LB = 0.83742\n",
    "* 18\n",
    "* Dropout=0.2/lr=2e-5/val=0.2/epochs=3/batch=16/anomall=True\n",
    "* LB = 0.83537\n",
    "* 25\n",
    "* Dropout=0.2/lr=3e-5/val=0.2/epochs=4/batch=32/anomall=False\n",
    "* LB = 0.83537\n",
    "* 37\n",
    "* without Dropout/lr=2e-5/val=0.2/epochs=4/batch=24/anomall=False\n",
    "* LB = 0.83435\n",
    "* 23\n",
    "* Dropout=0.2/lr=2e-5/val=0.2/epochs=5/batch=32/anomall=False\n",
    "* LB = 0.83435\n",
    "* 30\n",
    "* Dropout=0.1/lr=1e-5/val=0.2/epochs=4/batch=32/anomall=False\n",
    "* LB = 0.83333\n",
    "* 33\n",
    "* Dropout=0.2/lr=2e-5/val=0.3/epochs=4/batch=32/anomall=False\n",
    "* LB = 0.83128\n",
    "* 14\n",
    "* Dropout=0.2/lr=2e-5/val=0.2/epochs=3/batch=32/anomall=True\n",
    "* LB = 0.83128\n",
    "* 28\n",
    "* Dropout=0.2/lr=5e-6/val=0.2/epochs=4/batch=32/anomall=False\n",
    "* LB = 0.82924\n",
    "* 26\n",
    "* Dropout=0.2/lr=4e-5/val=0.2/epochs=4/batch=32/anomall=False\n",
    "* LB = 0.82617\n",
    "* 20\n",
    "* without Dropout/lr=1e-5/val=0.2/epochs=3/batch=16/anomall=True\n",
    "* LB = 0.82413\n",
    "* 32\n",
    "* Dropout=0.2/lr=2e-5/val=0.2/epochs=4/batch=32/anomall=True\n",
    "* LB = 0.82004\n",
    "* 37\n",
    "* without Dropout/lr=2e-5/val=0.2/epochs=3/batch=32/anomall=False\n",
    "* LB = 0.81390\n",
    "* 35\n",
    "* Dropout=0.15/lr=2e-5/val=0.2/epochs=4/batch=32/anomall=False\n",
    "* LB = 0.81390\n",
    "* 22\n",
    "* Dropout=0.2/lr=1.1e-5/val=0.2/epochs=3/batch=16/anomall=True\n",
    "* LB = 0.80879\n",
    "* 24\n",
    "* Dropout=0.3/lr=2e-5/val=0.2/epochs=4/batch=32/anomall=False\n",
    "* LB = 0.80470\n",
    "* 31\n",
    "* Dropout=0.2/lr=1e-5/val=0.15/epochs=4/batch=32/anomall=False\n",
    "* LB = 0.79856\n",
    "* 27\n",
    "* Dropout=0.2/lr=5e-4/val=0.2/epochs=4/batch=32/anomall=False\n",
    "* LB = 0.57055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit 39\n",
    "* without Dropout\n",
    "* Adam(lr=2e-5)\n",
    "* validation_split = 0.2,\n",
    "* epochs = 4,\n",
    "* batch_size = 24\n",
    "\n",
    "**LB = 0.83435**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download data <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet= pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data - \n",
    "# # author of this kernel read tweets in training data and figure out that some of them have errors:\n",
    "# ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "# tweet.loc[tweet['id'].isin(ids_with_target_error),'target'] = 0\n",
    "# tweet[tweet['id'].isin(ids_with_target_error)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\n",
    "print('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EDA <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to:\n",
    "* https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n",
    "* https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n",
    "* https://www.kaggle.com/itratrahman/nlp-tutorial-using-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin with anything else, let's check the class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the number of examples of each class\n",
    "Real_len = tweet[tweet['target'] == 1].shape[0]\n",
    "Not_len = tweet[tweet['target'] == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of the 3 classes\n",
    "plt.rcParams['figure.figsize'] = (7, 5)\n",
    "plt.bar(10,Real_len,3, label=\"Real\", color='blue')\n",
    "plt.bar(15,Not_len,3, label=\"Not\", color='red')\n",
    "plt.legend()\n",
    "plt.ylabel('Number of examples')\n",
    "plt.title('Propertion of examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of characters in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length(text):    \n",
    "    '''a function which returns the length of text'''\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['length'] = tweet['text'].apply(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18.0, 6.0)\n",
    "bins = 150\n",
    "plt.hist(tweet[tweet['target'] == 0]['length'], alpha = 0.6, bins=bins, label='Not')\n",
    "plt.hist(tweet[tweet['target'] == 1]['length'], alpha = 0.8, bins=bins, label='Real')\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('numbers')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlim(0,150)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "tweet_len=tweet[tweet['target']==1]['text'].str.len()\n",
    "ax1.hist(tweet_len,color='blue')\n",
    "ax1.set_title('disaster tweets')\n",
    "tweet_len=tweet[tweet['target']==0]['text'].str.len()\n",
    "ax2.hist(tweet_len,color='red')\n",
    "ax2.set_title('Not disaster tweets')\n",
    "fig.suptitle('Characters in tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "tweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\n",
    "ax1.hist(tweet_len,color='blue')\n",
    "ax1.set_title('disaster tweets')\n",
    "tweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\n",
    "ax2.hist(tweet_len,color='red')\n",
    "ax2.set_title('Not disaster tweets')\n",
    "fig.suptitle('Words in a tweet')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Average word length in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "word=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')\n",
    "ax1.set_title('disaster')\n",
    "word=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\n",
    "ax2.set_title('Not disaster')\n",
    "fig.suptitle('Average word length in each tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(target):\n",
    "    corpus=[]\n",
    "    \n",
    "    for x in tweet[tweet['target']==target]['text'].str.split():\n",
    "        for i in x:\n",
    "            corpus.append(i)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_df(tweet, target):\n",
    "    corpus=[]\n",
    "    \n",
    "    for x in tweet[tweet['target']==target]['text'].str.split():\n",
    "        for i in x:\n",
    "            corpus.append(i)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common stopwords in tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we  will analyze tweets with class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=create_corpus(0)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word]+=1\n",
    "        \n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the stopwords\n",
    "np.array(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (18.0, 6.0)\n",
    "x,y=zip(*top)\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,we will analyze tweets with class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=create_corpus(1)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop:\n",
    "        dic[word]+=1\n",
    "\n",
    "top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n",
    "    \n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18.0, 6.0)\n",
    "x,y=zip(*top)\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's check tweets indicating real disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "corpus=create_corpus(1)\n",
    "\n",
    "dic=defaultdict(int)\n",
    "special = string.punctuation\n",
    "for i in (corpus):\n",
    "    if i in special:\n",
    "        dic[i]+=1\n",
    "        \n",
    "x,y=zip(*dic.items())\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,we will move on to class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "corpus=create_corpus(0)\n",
    "dic=defaultdict(int)\n",
    "special = string.punctuation\n",
    "for i in (corpus):\n",
    "    if i in special:\n",
    "        dic[i]+=1\n",
    "        \n",
    "x,y=zip(*dic.items())\n",
    "plt.bar(x,y,color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "counter=Counter(corpus)\n",
    "most=counter.most_common()\n",
    "x=[]\n",
    "y=[]\n",
    "for word,count in most[:40]:\n",
    "    if (word not in stop) :\n",
    "        x.append(word)\n",
    "        y.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lot of cleaning needed !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will do a bigram (n=2) analysis over the tweets. Let's check the most common bigrams in tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tweet_bigrams(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "top_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:10]\n",
    "x,y=map(list,zip(*top_tweet_bigrams))\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([tweet,test])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "remove_URL(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_URL(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"<div>\n",
    "<h1>Real or Fake</h1>\n",
    "<p>Kaggle </p>\n",
    "<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n",
    "</div>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "print(remove_html(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_html(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoji(\"Omg another Earthquake ðŸ˜”ðŸ˜”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "example=\"I am a #king\"\n",
    "print(remove_punct(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. WordCloud <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_new1=create_corpus_df(df,1)\n",
    "len(corpus_new1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_new1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the wordcloud with the values under the category dataframe\n",
    "plt.figure(figsize=(12,8))\n",
    "word_cloud = WordCloud(\n",
    "                          background_color='black',\n",
    "                          max_font_size = 80\n",
    "                         ).generate(\" \".join(corpus_new1[:50]))\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_new0=create_corpus_df(df,0)\n",
    "len(corpus_new0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_new0[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the wordcloud with the values under the category dataframe\n",
    "plt.figure(figsize=(12,8))\n",
    "word_cloud = WordCloud(\n",
    "                          background_color='black',\n",
    "                          max_font_size = 80\n",
    "                         ).generate(\" \".join(corpus_new0[:50]))\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bag of Words Counts <a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "\n",
    "    return emb, count_vectorizer\n",
    "\n",
    "list_corpus = df[\"text\"].tolist()\n",
    "list_labels = df[\"target\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n",
    "                                                                                random_state=40)\n",
    "\n",
    "X_train_counts, count_vectorizer = cv(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n",
    "        lsa = TruncatedSVD(n_components=2)\n",
    "        lsa.fit(test_data)\n",
    "        lsa_scores = lsa.transform(test_data)\n",
    "        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n",
    "        color_column = [color_mapper[label] for label in test_labels]\n",
    "        colors = ['orange','blue']\n",
    "        if plot:\n",
    "            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "            orange_patch = mpatches.Patch(color='orange', label='Not')\n",
    "            blue_patch = mpatches.Patch(color='blue', label='Real')\n",
    "            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(X_train_counts, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These embeddings don't look very cleanly separated. Let's see if we can still fit a useful model on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TF IDF <a class=\"anchor\" id=\"8\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer\n",
    "\n",
    "X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(X_train_tfidf, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. GloVe <a class=\"anchor\" id=\"9\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use GloVe pretrained corpus model to represent our words. It is available in 3 varieties : 50D, 100D and 200 Dimentional. We will try 100D here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_new(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet)]\n",
    "        corpus.append(words)\n",
    "    return corpus   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=create_corpus_new(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word = values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i < num_words:\n",
    "        emb_vec=embedding_dict.get(word)\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[i]=emb_vec           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_pad[0][0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model with GloVe results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=3e-4)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tweet_pad[:tweet.shape[0]]\n",
    "test=tweet_pad[tweet.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.2)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 16))          \n",
    "plot_LSA(train,tweet['target'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomended 10-20 epochs\n",
    "history=model.fit(X_train,y_train,batch_size=4,epochs=10,validation_data=(X_test,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_GloVe = model.predict(train)\n",
    "train_pred_GloVe_int = train_pred_GloVe.round().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. BERT using TFHub <a class=\"anchor\" id=\"10\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to very good kernels https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the official tokenization script created by the Google team\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    #x = Dropout(0.2)(clf_output)\n",
    "    #out = Dense(1, activation='sigmoid')(x)\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT from the Tensorflow Hub\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files containing training data\n",
    "train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data - \n",
    "# author of this kernel read tweets in training data and figure out that some of them have errors:\n",
    "# ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "# train.loc[train['id'].isin(ids_with_target_error),'target'] = 0\n",
    "# train[train['id'].isin(ids_with_target_error)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer from the bert layer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the text into tokens, masks, and segment flags\n",
    "train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
    "train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: Build, Train, Predict, Submit\n",
    "model_BERT = build_model(bert_layer, max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "train_history = model_BERT.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split = 0.2,\n",
    "    epochs = 5, # recomended 3-5 epochs\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_BERT = model_BERT.predict(test_input)\n",
    "test_pred_BERT_int = test_pred_BERT.round().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_BERT = model_BERT.predict(train_input)\n",
    "train_pred_BERT_int = train_pred_BERT.round().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1. Submission by BERT<a class=\"anchor\" id=\"10.1\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(test_pred_BERT, columns=['preds'])\n",
    "pred.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = test_pred_BERT_int\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Showing Confusion Matrices<a class=\"anchor\" id=\"11\"></a>\n",
    "\n",
    "[Back to Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing Confusion Matrix\n",
    "def plot_cm(y_true, y_pred, title, figsize=(5,4)):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing Confusion Matrix for GloVe model\n",
    "plot_cm(train_pred_GloVe_int, train['target'].values, 'Confusion matrix for GloVe model', figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing Confusion Matrix for BERT model\n",
    "plot_cm(train_pred_BERT_int, train['target'].values, 'Confusion matrix for BERT model', figsize=(7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you find this kernel useful and enjoyable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your comments and feedback are most welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to Top](#0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
